{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/04-chat-memory.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZdSobItaTI"
      },
      "source": [
        "### LangChain Essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGqZi42taTJ"
      },
      "source": [
        "# Conversational Memory for OpenAI - LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkg11xv9taTK"
      },
      "source": [
        "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
        "\n",
        "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ydYy0Wc7tfLl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-openai==0.3.3 \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIJ-etvtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euKN4BNCtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSfe1JIwtaTK",
        "outputId": "8afe3d02-2f49-4dac-d3e8-b8968f690074"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-chat-memory-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ57c5BxtaTL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYjdtcPtaTL"
      },
      "source": [
        "## LangChain's Memory Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czt55jO1taTL"
      },
      "source": [
        "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
        "\n",
        "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
        "\n",
        "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
        "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
        "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
        "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
        "\n",
        "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brmml7G-taTL"
      },
      "source": [
        "## Initialize our LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1vhQcsxtaTL"
      },
      "source": [
        "Before jumping into our memory types, let's initialize our LLM. We will use OpenAI's `gpt-4o-mini` model, if you need an API key you can get one from [OpenAI's website](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNMunYQZtaTL",
        "outputId": "0d8735a5-68fb-476b-9ca9-201f8a4104bb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "    getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRVD0HctaTM"
      },
      "source": [
        "## 1. `ConversationBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40zJzXXXtaTM"
      },
      "source": [
        "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM.\n",
        "\n",
        "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY-BxLW3taTM",
        "outputId": "96f88376-cf9a-45f3-bde1-90fbf896022e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z7/20xbnn554f9b6jc8td1jyhjr0000gn/T/ipykernel_71347/1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkCBXxitaTM"
      },
      "source": [
        "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key). So, to create the following conversation:\n",
        "\n",
        "```\n",
        "User: Hi, my name is James\n",
        "AI: Hey James, what's up? I'm an AI model called Zeta.\n",
        "User: I'm researching the different types of conversational memory.\n",
        "AI: That's interesting, what are some examples?\n",
        "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "User: Buffer memory just stores the entire conversation, right?\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "User: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n",
        "\n",
        "We do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mUPUE9DhtaTM"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Hi, my name is James\"},  # user message\n",
        "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
        "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
        "    {\"output\": \"Very cool!\"}  # AI response\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6C4W54taTM"
      },
      "source": [
        "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRLwtZvWtaTM",
        "outputId": "c7781545-b5c9-4d5e-cb26-745dc3264c84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_qjbVustaTM"
      },
      "source": [
        "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the `add_user_message` and `add_ai_message` methods. To reproduce what we did above, we do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjy4g_GtaTM",
        "outputId": "4447488e-aa8c-4b5f-c2f6-11744453f777"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1tkc6DOtaTM"
      },
      "source": [
        "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwgjzC0ktaTM",
        "outputId": "231215d1-568b-4546-daa1-f12da4e47475"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z7/20xbnn554f9b6jc8td1jyhjr0000gn/T/ipykernel_71347/839683240.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  chain = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1fMZLWtaTN",
        "outputId": "6e185883-44a8-4538-e068-be8f54183d00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/joshuabriggs/Documents/GitHub/langchain-course/.venv/lib/python3.12/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=980180b3-ba58-4389-ac86-2152e093339b,id=980180b3-ba58-4389-ac86-2152e093339b; trace=980180b3-ba58-4389-ac86-2152e093339b,id=94f775e6-d74d-41a3-b9f7-8f1109cf79de\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Your name is James! How's your research going?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Your name is James! How's your research going?\"}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQTzPaovtaTN"
      },
      "source": [
        "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZa82dhtaTN"
      },
      "source": [
        "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
        "\n",
        "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UijZzCtutaTN"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnEAavfntaTN"
      },
      "source": [
        "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ywxify04taTN"
      },
      "outputs": [],
      "source": [
        "pipeline = prompt_template | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jhOF-uVtaTN"
      },
      "source": [
        "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BXKmcliGtaTN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39LBschEtaTN"
      },
      "source": [
        "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zHQRgNfrtaTO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwEeNAVtaTO"
      },
      "source": [
        "Now we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ESDoue1taTO",
        "outputId": "7f5606e2-dee4-4b59-e828-e8cca77b0bad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=8560071b-8d25-4fb5-b382-b54875be153f,id=8560071b-8d25-4fb5-b382-b54875be153f; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=ef47c2f0-f795-4dee-a86e-18034ed0661a; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=c58c46c5-8d58-449c-99e0-6928d3f37cff; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=8ef7fd37-a2ce-4428-bd85-916d2e317e7e; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=e2f3155d-168a-4edd-bccb-bf412e9b243e; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=70c1267e-1c5d-4eba-b007-a2501af6a828; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=f2f0cc1c-059b-4188-a194-033a160aa547; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=c449a988-fabd-457b-94e5-4e545097e2e9; trace=980180b3-ba58-4389-ac86-2152e093339b,id=94f775e6-d74d-41a3-b9f7-8f1109cf79de; trace=980180b3-ba58-4389-ac86-2152e093339b,id=980180b3-ba58-4389-ac86-2152e093339b\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=8560071b-8d25-4fb5-b382-b54875be153f,id=4e346820-e297-447d-808d-ca222330c612; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=3b48a467-434f-497e-bc5a-8896e5624f7c; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=f88ba4fe-8c95-4d35-90cc-f983b854cd4f; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=43506e00-a1d8-4270-86fb-529916fdeff3; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=d5c7b5c4-4f54-4a7b-b085-25d4490a4487; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=54a9720e-3a0d-48ad-9661-c4d84a67162f; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=1e670ba1-34c6-4577-a866-6cb90cfb0541; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=77cea3a6-62b6-46bf-b811-5ef3a126cb75; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=887c3d28-858f-4368-9e73-35ccc8f12bc8; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=8560071b-8d25-4fb5-b382-b54875be153f; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=e2f3155d-168a-4edd-bccb-bf412e9b243e; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=70c1267e-1c5d-4eba-b007-a2501af6a828; trace=8560071b-8d25-4fb5-b382-b54875be153f,id=c449a988-fabd-457b-94e5-4e545097e2e9\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=c79e944f-4bee-450e-aa12-795532dde67f; trace=433953b2-1123-4016-b1af-89842a2273bf,id=433953b2-1123-4016-b1af-89842a2273bf; trace=433953b2-1123-4016-b1af-89842a2273bf,id=4169f116-1679-4515-9e3b-b6615d1e9518; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=887c3d28-858f-4368-9e73-35ccc8f12bc8; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=3b48a467-434f-497e-bc5a-8896e5624f7c; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=54a9720e-3a0d-48ad-9661-c4d84a67162f; trace=3b48a467-434f-497e-bc5a-8896e5624f7c,id=1e670ba1-34c6-4577-a866-6cb90cfb0541\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=1803900c-896c-4b56-8ef1-1a66de5750e2; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=b02766cd-4e2d-4eb7-9c1a-e02cdd47f4ef; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=f8c06f7b-fe93-4489-b615-b80a663087cb; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=d18c9c6d-65db-4c17-8168-282d8bc92d23; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=defcd499-a4b8-481e-a94d-4a1a79c9cf9e; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=8acb0c7d-32e4-47f1-90b7-337e5224595a; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=8be65ec5-45f0-443d-ba7b-6ff0c122444f; trace=433953b2-1123-4016-b1af-89842a2273bf,id=4169f116-1679-4515-9e3b-b6615d1e9518; trace=433953b2-1123-4016-b1af-89842a2273bf,id=433953b2-1123-4016-b1af-89842a2273bf\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=28a512ba-265d-4f8f-84cf-6da96abe0528; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=938b0c0f-d612-4c27-b3d4-b2448a9b80d0; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=c92bcbe7-29c3-4718-aca2-095df49f5861; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=da0a1a02-42fe-4d44-9dd5-1148c08a70a2; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=69f751f9-515b-4daa-a5d4-ab8cc7f52a0c; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=eea2e985-e3bb-4cbd-bd87-cccdcb7bdbb8; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=416578cf-b516-4023-8fde-b92e59cf2b46; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=c03e3c1a-2093-489d-b70c-ca6abfbbeb17; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=95b98482-05e3-4d3c-b20c-585a7bf628d6; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=8be65ec5-45f0-443d-ba7b-6ff0c122444f; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=d18c9c6d-65db-4c17-8168-282d8bc92d23; trace=b449eb10-c9da-4a44-8abd-e2a4e4cd88fe,id=defcd499-a4b8-481e-a94d-4a1a79c9cf9e\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=4c3fbbe3-1ed2-41f7-b1a1-46f8399bfd9a; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=bf0b2396-758f-4424-bb17-54d25f741f09; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=9b1f81af-c193-4237-92f0-0d0e21e452f8; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=485e780e-af99-4344-b618-f09e8aa9940b; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=55a514e9-f595-49c7-83eb-319b93a00033; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=88c35932-45e6-4c8e-8d9b-15911c995443; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=83f2685a-a9f8-415b-8d78-2f8c1c92cd2b; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=f8ffa7bc-102f-47a1-9782-39d50f79941b; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=eae8cd05-2750-43ed-b973-22b37c97718d; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=95b98482-05e3-4d3c-b20c-585a7bf628d6; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=938b0c0f-d612-4c27-b3d4-b2448a9b80d0; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=eea2e985-e3bb-4cbd-bd87-cccdcb7bdbb8; trace=938b0c0f-d612-4c27-b3d4-b2448a9b80d0,id=416578cf-b516-4023-8fde-b92e59cf2b46\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=da84e663-8aca-402f-a5d1-7e01c125ca33; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=a6b47431-51f9-4a2f-bc83-f841590f2065; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=5ac6ec11-7a3e-40f5-b615-15acb7ba1574; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=6a5df295-facc-46d3-95e2-f3843e9ed1e5; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=057ac482-998c-46b1-a920-c8d8461b997e; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=59bd8f95-e1de-4a22-ab2c-f14394c7d50d; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=5a9e886b-5465-4095-acce-30beba8fe53f; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=614999ec-5d39-4e1f-85fc-7e38c5bb2d11; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=bf0b2396-758f-4424-bb17-54d25f741f09; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=88c35932-45e6-4c8e-8d9b-15911c995443; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=83f2685a-a9f8-415b-8d78-2f8c1c92cd2b; trace=bf0b2396-758f-4424-bb17-54d25f741f09,id=eae8cd05-2750-43ed-b973-22b37c97718d\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=0ae8d8d4-7cb2-42e2-8c3a-260d4e4f87d0; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=614999ec-5d39-4e1f-85fc-7e38c5bb2d11; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=057ac482-998c-46b1-a920-c8d8461b997e; trace=5b710ea4-3e7f-4ae0-bba7-bad1c5d18003,id=59bd8f95-e1de-4a22-ab2c-f14394c7d50d\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=8f67ecc3-8b23-431f-a915-016925850dbe,id=8f67ecc3-8b23-431f-a915-016925850dbe; trace=8f67ecc3-8b23-431f-a915-016925850dbe,id=a566a8bc-2682-4dfb-8a30-0c2761a69d72\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=b283b3f8-172d-4b40-aa63-4df8d1c4890d,id=b283b3f8-172d-4b40-aa63-4df8d1c4890d; trace=b283b3f8-172d-4b40-aa63-4df8d1c4890d,id=a671e1c4-3fa2-41ff-8f57-33dd98bab140; trace=8f67ecc3-8b23-431f-a915-016925850dbe,id=a566a8bc-2682-4dfb-8a30-0c2761a69d72\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=0b4a1f5e-50a3-4127-b1eb-987fac587826,id=0b4a1f5e-50a3-4127-b1eb-987fac587826; trace=0b4a1f5e-50a3-4127-b1eb-987fac587826,id=aaf88b56-b625-4ee7-b925-8033ff1717ea; trace=b283b3f8-172d-4b40-aa63-4df8d1c4890d,id=b283b3f8-172d-4b40-aa63-4df8d1c4890d; trace=b283b3f8-172d-4b40-aa63-4df8d1c4890d,id=a671e1c4-3fa2-41ff-8f57-33dd98bab140; trace=8f67ecc3-8b23-431f-a915-016925850dbe,id=8f67ecc3-8b23-431f-a915-016925850dbe\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=222743d7-7dcd-4f03-9dd6-dc43f1cebb24,id=222743d7-7dcd-4f03-9dd6-dc43f1cebb24; trace=222743d7-7dcd-4f03-9dd6-dc43f1cebb24,id=3f56faff-f9dc-4596-bbb8-2b78ec0bcefc; trace=0b4a1f5e-50a3-4127-b1eb-987fac587826,id=aaf88b56-b625-4ee7-b925-8033ff1717ea\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=1d03ebfe-b0b3-48fc-a4e8-f3d27fbfa1c3,id=1d03ebfe-b0b3-48fc-a4e8-f3d27fbfa1c3; trace=1d03ebfe-b0b3-48fc-a4e8-f3d27fbfa1c3,id=9ab631aa-2dae-4a44-b2da-fe062990a9e2; trace=222743d7-7dcd-4f03-9dd6-dc43f1cebb24,id=222743d7-7dcd-4f03-9dd6-dc43f1cebb24; trace=222743d7-7dcd-4f03-9dd6-dc43f1cebb24,id=3f56faff-f9dc-4596-bbb8-2b78ec0bcefc; trace=0b4a1f5e-50a3-4127-b1eb-987fac587826,id=0b4a1f5e-50a3-4127-b1eb-987fac587826\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=f4d0c3eb-2473-4fe3-9d4e-76c3579c4b6e,id=f4d0c3eb-2473-4fe3-9d4e-76c3579c4b6e; trace=f4d0c3eb-2473-4fe3-9d4e-76c3579c4b6e,id=928033bc-5688-49f8-b56f-b03436e53479; trace=1d03ebfe-b0b3-48fc-a4e8-f3d27fbfa1c3,id=9ab631aa-2dae-4a44-b2da-fe062990a9e2\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=62553c4f-3b9f-48a5-a05b-9a122cab3f42,id=62553c4f-3b9f-48a5-a05b-9a122cab3f42; trace=62553c4f-3b9f-48a5-a05b-9a122cab3f42,id=5e87106d-6605-4b57-a918-1202f31a0a06; trace=f4d0c3eb-2473-4fe3-9d4e-76c3579c4b6e,id=f4d0c3eb-2473-4fe3-9d4e-76c3579c4b6e; trace=f4d0c3eb-2473-4fe3-9d4e-76c3579c4b6e,id=928033bc-5688-49f8-b56f-b03436e53479; trace=1d03ebfe-b0b3-48fc-a4e8-f3d27fbfa1c3,id=1d03ebfe-b0b3-48fc-a4e8-f3d27fbfa1c3\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=ceddff65-aab3-450a-be2d-2d1f30034ede,id=ceddff65-aab3-450a-be2d-2d1f30034ede; trace=ceddff65-aab3-450a-be2d-2d1f30034ede,id=a3cb45f2-28a0-447e-9640-8b490745a527; trace=62553c4f-3b9f-48a5-a05b-9a122cab3f42,id=5e87106d-6605-4b57-a918-1202f31a0a06\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=b45c2b05-f964-486a-8f31-7282bede560e,id=b45c2b05-f964-486a-8f31-7282bede560e; trace=b45c2b05-f964-486a-8f31-7282bede560e,id=88c1f439-ccf8-4aaa-83d1-f46f4297d6b7; trace=ceddff65-aab3-450a-be2d-2d1f30034ede,id=ceddff65-aab3-450a-be2d-2d1f30034ede; trace=ceddff65-aab3-450a-be2d-2d1f30034ede,id=a3cb45f2-28a0-447e-9640-8b490745a527; trace=62553c4f-3b9f-48a5-a05b-9a122cab3f42,id=62553c4f-3b9f-48a5-a05b-9a122cab3f42\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=55d2d3a1-5ade-44fb-bf3f-8ec06c9e7f6c,id=55d2d3a1-5ade-44fb-bf3f-8ec06c9e7f6c; trace=55d2d3a1-5ade-44fb-bf3f-8ec06c9e7f6c,id=fb41305f-bad3-4e62-b1b4-140d3b7f547b; trace=b45c2b05-f964-486a-8f31-7282bede560e,id=88c1f439-ccf8-4aaa-83d1-f46f4297d6b7\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=1a2e2681-6a26-463a-84a6-ddbb76340cd1,id=1a2e2681-6a26-463a-84a6-ddbb76340cd1; trace=1a2e2681-6a26-463a-84a6-ddbb76340cd1,id=6a11a64e-6f26-47ac-b2d0-3cc8f86d3a4c; trace=55d2d3a1-5ade-44fb-bf3f-8ec06c9e7f6c,id=55d2d3a1-5ade-44fb-bf3f-8ec06c9e7f6c; trace=55d2d3a1-5ade-44fb-bf3f-8ec06c9e7f6c,id=fb41305f-bad3-4e62-b1b4-140d3b7f547b; trace=b45c2b05-f964-486a-8f31-7282bede560e,id=b45c2b05-f964-486a-8f31-7282bede560e\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=43ff707e-b436-4c89-ab41-5afb8183b029,id=43ff707e-b436-4c89-ab41-5afb8183b029; trace=43ff707e-b436-4c89-ab41-5afb8183b029,id=8fbdd6a0-4f1d-48ac-86d0-3798f2846105; trace=1a2e2681-6a26-463a-84a6-ddbb76340cd1,id=6a11a64e-6f26-47ac-b2d0-3cc8f86d3a4c\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=f3d63ccb-9ac5-495a-b57b-15b2b915e734; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=9037a8f4-ea0f-4e0f-be80-161e22f47a8f; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=59912727-a1e1-4804-9be4-0bd8cd29a0ba; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=5ee7307e-3a4f-4fd0-9f54-44f402b03227; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=10116133-d800-462d-962f-dd77ee7527f9; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=9e38473f-ef93-44ee-acd0-fd8131d5c898; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=3c63b53f-cf38-4cf0-aed4-83301d3778d0; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=a38a0644-74e2-4b97-8e6f-9bd81d2a1369; trace=43ff707e-b436-4c89-ab41-5afb8183b029,id=43ff707e-b436-4c89-ab41-5afb8183b029; trace=43ff707e-b436-4c89-ab41-5afb8183b029,id=8fbdd6a0-4f1d-48ac-86d0-3798f2846105; trace=1a2e2681-6a26-463a-84a6-ddbb76340cd1,id=1a2e2681-6a26-463a-84a6-ddbb76340cd1\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=ec1c70a5-4f56-4354-b74f-056485e697f9; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=a11a6832-9083-4013-8e93-3337ee1c3f84; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=b6e4f5ad-afe3-4d98-b380-3f0a1df7acec; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=ecdd6ccb-bfeb-47ad-8830-578051c1f87e; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=a46683c8-35b6-45c6-95e6-62aef5dc62a2; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=7b889d38-7fba-4c0d-b9a0-45feea65730a; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=681838f8-8bd3-4661-aa71-0006898eba48; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=3357adbd-de5b-4019-8251-5feb02f95ef4; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=24072554-9c94-4adc-a34f-63061d4dc466; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=9e38473f-ef93-44ee-acd0-fd8131d5c898; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=a38a0644-74e2-4b97-8e6f-9bd81d2a1369; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=f3d63ccb-9ac5-495a-b57b-15b2b915e734; trace=f3d63ccb-9ac5-495a-b57b-15b2b915e734,id=10116133-d800-462d-962f-dd77ee7527f9\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=6489ad62-e71c-4c1c-8bb6-d152989d3189; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=631d70d1-5fc3-4686-92f5-15f798096b68; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=8215f5bd-12c1-431a-ba25-69489a94b243; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=53eee4a9-1984-4b5c-96e3-d36448863221; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=f007c836-951f-455f-b0cc-2b136ed6a42a; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=b12e7665-ef79-4aca-a83e-36e93ef223eb; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=a6d1f80f-c560-4ca1-8f16-3bbbb6b96edc; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=43572c8c-9071-4753-9385-993606935d14; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=24072554-9c94-4adc-a34f-63061d4dc466; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=681838f8-8bd3-4661-aa71-0006898eba48; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=a11a6832-9083-4013-8e93-3337ee1c3f84; trace=a11a6832-9083-4013-8e93-3337ee1c3f84,id=7b889d38-7fba-4c0d-b9a0-45feea65730a\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=5a827d68-26bc-4d6e-8600-b77913d56014; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=9e0498c0-f8f5-45e1-9ac5-a0eba910e229; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=ea7180f5-99ec-4f55-b65a-6622df2b8703; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=3d7a6813-7f4b-4c55-935f-ad5dcad3d32b; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=01053bf6-882c-4262-99c6-284776ca5ed2; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=31c084a8-cf41-4b18-a3ba-4d24d5c377e2; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=c8ea4330-c02d-47b1-8442-c448f64587ac; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=9202539d-543e-4cb4-9045-6cbf5b499093; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=24655ef9-0025-4ae0-b481-ea72fae17d82; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=b12e7665-ef79-4aca-a83e-36e93ef223eb; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=43572c8c-9071-4753-9385-993606935d14; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4; trace=a4c3b9f8-19b7-4cf2-b7ff-e9f1169baee4,id=f007c836-951f-455f-b0cc-2b136ed6a42a\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=eadc756a-deac-45c8-812e-59fe47340214; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=0cca4635-d782-4a11-8f07-f2e75f8e70ea; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=a6211d4c-c1d4-4774-9516-1e7b8c810748; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=322108c6-ec2a-495d-a59e-41b3cea22d68; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=c1bcd63f-f617-4b65-81ac-b441053176a7; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=d67a4409-881a-4a07-8f73-d7f22ae0a5a4; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=ecbb41b8-ab5c-4777-91ea-b9b68ee2f851; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=bd6452eb-e532-4bec-b650-57f396ebeb1b; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=6d8aa0f0-f3a0-41fd-9d59-b432d245cd43; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=c8ea4330-c02d-47b1-8442-c448f64587ac; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=24655ef9-0025-4ae0-b481-ea72fae17d82; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=9e0498c0-f8f5-45e1-9ac5-a0eba910e229; trace=9e0498c0-f8f5-45e1-9ac5-a0eba910e229,id=31c084a8-cf41-4b18-a3ba-4d24d5c377e2\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=24a77dc4-02e4-44bb-8b13-ddf9c5ac9d2e; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=0749e474-9ff0-42c3-8999-12285f83dcf1; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=481cb890-0787-4e57-9554-b2486f3ca1ef; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=45725633-dd0b-432f-8927-0286ba368dec; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=ebf82fe8-01fe-4c1d-8fad-27fefb92fe67; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=3df30682-802c-4c17-a2de-70b00649da0c; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=e3186a52-2d79-438f-b7e1-a5fdab0f0588; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=393d6389-81da-4f68-8a48-c47cfef8efa4; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=c3551e24-7018-4904-9106-d59fdceafa12; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=6d8aa0f0-f3a0-41fd-9d59-b432d245cd43; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=ecbb41b8-ab5c-4777-91ea-b9b68ee2f851; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=0cca4635-d782-4a11-8f07-f2e75f8e70ea; trace=0cca4635-d782-4a11-8f07-f2e75f8e70ea,id=d67a4409-881a-4a07-8f73-d7f22ae0a5a4\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=98230789-8780-4ca5-8609-334e3514381c; trace=15fbba6b-3dc5-4200-888e-912491a41746,id=15fbba6b-3dc5-4200-888e-912491a41746; trace=15fbba6b-3dc5-4200-888e-912491a41746,id=e8f9cf52-1233-4445-97fd-8b6a71763595; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=c3551e24-7018-4904-9106-d59fdceafa12; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=e3186a52-2d79-438f-b7e1-a5fdab0f0588; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=0749e474-9ff0-42c3-8999-12285f83dcf1; trace=0749e474-9ff0-42c3-8999-12285f83dcf1,id=3df30682-802c-4c17-a2de-70b00649da0c\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=15fbba6b-3dc5-4200-888e-912491a41746,id=e8f9cf52-1233-4445-97fd-8b6a71763595\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=79eb33f9-0c3d-45c9-92d4-8b89edcc08a9,id=79eb33f9-0c3d-45c9-92d4-8b89edcc08a9; trace=79eb33f9-0c3d-45c9-92d4-8b89edcc08a9,id=39697112-59e1-4071-811b-ceff3daba196; trace=15fbba6b-3dc5-4200-888e-912491a41746,id=15fbba6b-3dc5-4200-888e-912491a41746\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=3ea2e23d-b40c-4138-a62f-b7314a6ee106,id=3ea2e23d-b40c-4138-a62f-b7314a6ee106; trace=3ea2e23d-b40c-4138-a62f-b7314a6ee106,id=57b52483-b33a-4188-aa3b-e34b14cb9af1; trace=79eb33f9-0c3d-45c9-92d4-8b89edcc08a9,id=39697112-59e1-4071-811b-ceff3daba196\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=3c92c7be-1412-45e6-86a3-fa60001a11fc,id=3c92c7be-1412-45e6-86a3-fa60001a11fc; trace=3c92c7be-1412-45e6-86a3-fa60001a11fc,id=05997507-db40-4e88-b2db-9ce0ca79cab1; trace=3ea2e23d-b40c-4138-a62f-b7314a6ee106,id=3ea2e23d-b40c-4138-a62f-b7314a6ee106; trace=3ea2e23d-b40c-4138-a62f-b7314a6ee106,id=57b52483-b33a-4188-aa3b-e34b14cb9af1; trace=79eb33f9-0c3d-45c9-92d4-8b89edcc08a9,id=79eb33f9-0c3d-45c9-92d4-8b89edcc08a9\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=22adc5e6-ec15-4b69-9cb2-596a21e88f5b,id=22adc5e6-ec15-4b69-9cb2-596a21e88f5b; trace=22adc5e6-ec15-4b69-9cb2-596a21e88f5b,id=0fb9af56-cbb5-492e-ae81-0ed028d8997f; trace=3c92c7be-1412-45e6-86a3-fa60001a11fc,id=05997507-db40-4e88-b2db-9ce0ca79cab1; trace=3c92c7be-1412-45e6-86a3-fa60001a11fc,id=3c92c7be-1412-45e6-86a3-fa60001a11fc\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=35bf633e-7afd-4bf6-8499-6a6916424513,id=35bf633e-7afd-4bf6-8499-6a6916424513; trace=35bf633e-7afd-4bf6-8499-6a6916424513,id=e8f407db-1312-41a2-9464-ed93e62894a6; trace=22adc5e6-ec15-4b69-9cb2-596a21e88f5b,id=0fb9af56-cbb5-492e-ae81-0ed028d8997f\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=8b28b7f1-9bd8-4c56-936b-0734088fda6e,id=8b28b7f1-9bd8-4c56-936b-0734088fda6e; trace=8b28b7f1-9bd8-4c56-936b-0734088fda6e,id=ae18b00d-6e6a-49a7-9358-e40915c0a2d7; trace=35bf633e-7afd-4bf6-8499-6a6916424513,id=35bf633e-7afd-4bf6-8499-6a6916424513; trace=35bf633e-7afd-4bf6-8499-6a6916424513,id=e8f407db-1312-41a2-9464-ed93e62894a6; trace=22adc5e6-ec15-4b69-9cb2-596a21e88f5b,id=22adc5e6-ec15-4b69-9cb2-596a21e88f5b\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=5268efb9-8de1-42e3-852d-558c3994c45d,id=5268efb9-8de1-42e3-852d-558c3994c45d; trace=5268efb9-8de1-42e3-852d-558c3994c45d,id=b85ed6dc-7458-4d41-8d0b-c9c530c87b9f; trace=8b28b7f1-9bd8-4c56-936b-0734088fda6e,id=ae18b00d-6e6a-49a7-9358-e40915c0a2d7\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=c2bbda4e-32b8-4164-af75-c40d4170caca; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=de6355b4-c42d-460f-95ee-db217d46f337; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=a787a2e8-2c82-475f-ae61-25be8f40d941; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=748efec6-e30d-466e-9c92-3b0c97541188; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=414e5bb1-2c0a-4c82-b1fa-24a8342aa9bf; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=d32317e8-ac2a-485c-9053-acba7ed0ea9c; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=e2eb70bd-25f6-4a3f-b90f-197a8aae8b1a; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=f185fb58-a405-4e36-84e0-b73dcd358466; trace=5268efb9-8de1-42e3-852d-558c3994c45d,id=5268efb9-8de1-42e3-852d-558c3994c45d; trace=5268efb9-8de1-42e3-852d-558c3994c45d,id=b85ed6dc-7458-4d41-8d0b-c9c530c87b9f; trace=8b28b7f1-9bd8-4c56-936b-0734088fda6e,id=8b28b7f1-9bd8-4c56-936b-0734088fda6e\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=a1c595ef-8330-4e24-8a3d-3f58fdb02e4f; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=1a9be471-b76e-45ce-89e5-8042a56cbf9a; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=c81d39cb-496a-4911-a935-80a4270f528d; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=36f3c2ce-141f-4936-b44c-33449f89f3a5; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=5f1f9eb4-639a-4a42-8b95-2023e7655ca0; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=659ece03-8204-4dde-9667-6f999683d314; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=0a60faca-05ba-45a7-be9a-aaf77a4919b1; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=0d4b1d94-9094-41a5-b093-fdda0d94e245; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=e9e3a2d3-04d0-4c48-80a9-9655bb2d0dad; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=f185fb58-a405-4e36-84e0-b73dcd358466; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=d32317e8-ac2a-485c-9053-acba7ed0ea9c; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=c2bbda4e-32b8-4164-af75-c40d4170caca; trace=c2bbda4e-32b8-4164-af75-c40d4170caca,id=414e5bb1-2c0a-4c82-b1fa-24a8342aa9bf\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=1b0905fe-38f2-427f-8130-dc56a21f5bd3; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=fafc1b8c-5845-40b9-97a7-c9ee8b547697; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=ae4922d5-2eb5-47d5-bbad-f6abba547b0b; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=5bfc6dcc-a888-4ec9-9fbe-39a838713434; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=8433044c-e8cb-4df0-bf99-506f58bfc327; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=1ad1af3f-e053-489d-9cd3-26d0f7dbc5aa; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=0b845cc5-6ad4-4e26-94aa-a8525b4e4d9d; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=fbe03c7f-5b5b-4937-b728-7f652be7422b; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=e9e3a2d3-04d0-4c48-80a9-9655bb2d0dad; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=0a60faca-05ba-45a7-be9a-aaf77a4919b1; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=1a9be471-b76e-45ce-89e5-8042a56cbf9a; trace=1a9be471-b76e-45ce-89e5-8042a56cbf9a,id=659ece03-8204-4dde-9667-6f999683d314\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=e277c3a8-a7bc-4bfa-8c26-55abf673ff53; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=a4d4f541-f3d4-4072-95f9-6fb9abd166fd; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=1ad1af3f-e053-489d-9cd3-26d0f7dbc5aa; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=fbe03c7f-5b5b-4937-b728-7f652be7422b\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=71a0a411-4ec7-4378-a70e-8f48dd4df575; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=057603f2-b46e-49c7-a258-f70cba90e49f; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=84b23014-1498-4da7-89e6-849be3bfe328; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=72700403-4c75-4476-b612-948ccef9a258; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=1d979b23-6a2a-4d4f-a7e9-3c9befd91e1a; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=adefe41b-43d3-4d42-8d43-36bceff48c10; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=2f970d9a-e8f0-4001-b4f8-867d7cc40f0e; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=a4d4f541-f3d4-4072-95f9-6fb9abd166fd; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d; trace=b1bc42f0-856e-4740-ad8f-08c8bfd41e4d,id=8433044c-e8cb-4df0-bf99-506f58bfc327\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=b3f9f78b-a6eb-495a-a7a4-ebc35827edb8; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=6d825fa0-0ff9-4c7c-abde-18b8928d417a; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=1d979b23-6a2a-4d4f-a7e9-3c9befd91e1a; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=2f970d9a-e8f0-4001-b4f8-867d7cc40f0e\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=62582f36-bb02-41d1-aa6b-8478008bafcb; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=29df1fe8-e1ca-40ba-b03a-84c0f6f1a1a9; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=d32c4f96-6477-40e1-950b-bd4e788e0d43; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=9b0fe3b8-1b7f-46b3-bcd4-cb65f49a5ef5; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=b8a19cf5-d8d7-454f-aea4-f1c773cd85e0; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=1ff31c19-9370-47c3-ad69-7a67bc649e6f; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=667ab31a-b717-46b3-90bc-e2de9917422d; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=2c5461b8-82a5-46b5-84e6-686bb5ba38b2; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=6d825fa0-0ff9-4c7c-abde-18b8928d417a; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76; trace=fdf8ff97-d5f0-4d1c-adc8-890d356f5b76,id=72700403-4c75-4476-b612-948ccef9a258\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=e03be4ee-b4db-4afe-a8a3-6e4ac6069e13; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=05b47cc6-5177-4ab7-9ccc-770ce1b50992; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=2c5461b8-82a5-46b5-84e6-686bb5ba38b2; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=1ff31c19-9370-47c3-ad69-7a67bc649e6f\n",
            "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=05b47cc6-5177-4ab7-9ccc-770ce1b50992; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=62582f36-bb02-41d1-aa6b-8478008bafcb; trace=62582f36-bb02-41d1-aa6b-8478008bafcb,id=b8a19cf5-d8d7-454f-aea4-f1c773cd85e0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c449a988-fabd-457b-94e5-4e545097e2e9-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avL-i76_taTO"
      },
      "source": [
        "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgfzWIBstaTO",
        "outputId": "960b9d46-f422-4486-d36b-e45623394c1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 50, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-887c3d28-858f-4368-9e73-35ccc8f12bc8-0', usage_metadata={'input_tokens': 50, 'output_tokens': 12, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjSuam-taTO"
      },
      "source": [
        "We have now recreated the `ConversationBufferMemory` type using the `RunnableWithMessageHistory` class. Let's continue onto other memory types and see how these can be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0MliXn-taTO"
      },
      "source": [
        "## 2. `ConversationBufferWindowMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lziswUkqtaTO"
      },
      "source": [
        "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
        "\n",
        "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
        "\n",
        "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
        "\n",
        "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
        "\n",
        "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBb46qDDtaTO",
        "outputId": "99e71fe7-48be-4579-d404-e5ec8c933e09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z7/20xbnn554f9b6jc8td1jyhjr0000gn/T/ipykernel_71347/3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to6rLd_ataTO"
      },
      "source": [
        "We populate this memory using the same methods as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pfxuLS7taTP",
        "outputId": "51772b66-bda9-406a-b6e3-d026c231f24f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV_6vOOwtaTP"
      },
      "source": [
        "As before, we use the `ConversationChain` object (again, this is deprecated and we will rewrite it with `RunnableWithMessageHistory` in a moment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "opmjk94ltaTP"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ-g586vtaTP"
      },
      "source": [
        "Now let's see if our LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWa5mhzftaTP",
        "outputId": "19a63db6-82ff-4ba3-84de-e3d4c42f3ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"I'm sorry, but I don't have access to your name or any personal information. If you'd like to share it, I can remember it for the duration of our conversation!\"}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GrFsMbStaTP"
      },
      "source": [
        "The reason our LLM can no longer remember our name is because we have set the `k` parameter to `4`, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves.\n",
        "\n",
        "Based on the agent forgetting our name, we might wonder _why_ we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyXf5nKtaTP"
      },
      "source": [
        "### `ConversationBufferWindowMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaUDp7GtaTP"
      },
      "source": [
        "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before. We define our `prompt_template` and `llm` as before, and then wrap our pipeline in a `RunnableWithMessageHistory` object.\n",
        "\n",
        "For the window feature, we need to define a custom version of the `InMemoryChatMessageHistory` class that removes any messages beyond the last `k` messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fQUv8yPltaTP"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "        super().__init__(k=k)\n",
        "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uoDJ38vztaTP"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "    # remove anything beyond the last\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GsXhm811taTP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQR8TZLGtaTQ"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `k` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBCnrghLtaTQ",
        "outputId": "05455096-12c0-40eb-8b62-eaaddeb5204d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n",
            "Initializing BufferWindowMessageHistory with k=4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-8be65ec5-45f0-443d-ba7b-6ff0c122444f-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AvqsaA3taTQ"
      },
      "source": [
        "We can also modify the messages that are stored in memory by modifying the records inside the `chat_map` dictionary directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDMrCS26taTQ",
        "outputId": "116bcb51-3585-404c-9e4a-0f5391b0136f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k4\"].clear()  # clear the history\n",
        "\n",
        "# manually insert history\n",
        "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k4\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzArAVDtaTQ"
      },
      "source": [
        "Now let's see at which `k` value our LLM remembers our name — from the above we can already see that with `k=4` our name is not mentioned, so when running with `k=4` we should expect the LLM to forget our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn3K6fxtaTQ",
        "outputId": "de1b7db8-214e-4936-d322-635b19571ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless you've shared it in this conversation. If you'd like, you can tell me your name!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 79, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-95b98482-05e3-4d3c-b20c-585a7bf628d6-0', usage_metadata={'input_tokens': 79, 'output_tokens': 32, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN7am67YtaTQ"
      },
      "source": [
        "Now let's initialize a new session with `k=14`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXU6nI0taTQ",
        "outputId": "77812ee2-d549-4321-b632-a268d5ad239c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n",
            "Initializing BufferWindowMessageHistory with k=14\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-eae8cd05-2750-43ed-b973-22b37c97718d-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPyLeEOtaTQ"
      },
      "source": [
        "We'll manually insert the remaining messages as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyHMnkCtaTQ",
        "outputId": "bc214490-f26e-4ee9-9763-2ff107743ba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-eae8cd05-2750-43ed-b973-22b37c97718d-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RVgWS6EtaTQ"
      },
      "source": [
        "Now let's see if the LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8X1xV9ItaTQ",
        "outputId": "45b6425c-8d94-4900-9c9d-4fb6db4583a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 156, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-614999ec-5d39-4e1f-85fc-7e38c5bb2d11-0', usage_metadata={'input_tokens': 156, 'output_tokens': 5, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3SqKjEvtaTQ",
        "outputId": "4ada095e-c9f5-4504-ceb6-cf7a40db677d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-eae8cd05-2750-43ed-b973-22b37c97718d-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 156, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-614999ec-5d39-4e1f-85fc-7e38c5bb2d11-0', usage_metadata={'input_tokens': 156, 'output_tokens': 5, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R91uVCqFtaTQ"
      },
      "source": [
        "That's it! We've rewritten our buffer window memory using the recommended `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK51EzantaTR"
      },
      "source": [
        "## 3. `ConversationSummaryMemory`\n",
        "\n",
        "Next up we have `ConversationSummaryMemory`, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
        "\n",
        "As before, we'll start with the original memory class before reimplementing it with the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1DRv1umtaTR",
        "outputId": "6d6f93b9-31fd-4313-8942-aa5d39546831"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z7/20xbnn554f9b6jc8td1jyhjr0000gn/T/ipykernel_71347/988334424.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDK1xTiQtaTR"
      },
      "source": [
        "Unlike with the previous memory types, we need to provide an `llm` to initialize `ConversationSummaryMemory`. The reason for this is that we need an LLM to generate the conversation summaries.\n",
        "\n",
        "Beyond this small tweak, using `ConversationSummaryMemory` is the same as with our previous memory types when using the deprecated `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "A5hxfssAtaTR"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nHb-SqtaTR"
      },
      "source": [
        "Let's test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwCtQA0RtaTR",
        "outputId": "0fe06aa4-e2dc-4b10-d994-296f8b642323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: hello there my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI greets James and expresses its willingness to chat and help, inquiring about how his day is going.\n",
            "Human: I am researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James, and the AI greets him, expressing its willingness to chat and help while inquiring about his day. James mentions he is researching different types of conversational memory, to which the AI responds by explaining that conversational memory involves how systems remember past interactions for a personalized experience. The AI outlines four types of conversational memory: short-term memory, which retains context within a single conversation; long-term memory, which retains information across multiple interactions; contextual memory, which focuses on understanding the flow of conversation; and dynamic memory, which updates based on new information or changes in user preferences. The AI then asks James if he is looking into any specific applications or technologies related to conversational memory.\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James, and the AI greets him, expressing its willingness to chat and help while inquiring about his day. James mentions he is researching different types of conversational memory, to which the AI responds by explaining that conversational memory involves how systems remember past interactions for a personalized experience. The AI outlines four types of conversational memory: short-term memory, which retains context within a single conversation; long-term memory, which retains information across multiple interactions; contextual memory, which focuses on understanding the flow of conversation; and dynamic memory, which updates based on new information or changes in user preferences. The AI then asks James if he is looking into any specific applications or technologies related to conversational memory. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI finds this interesting and explains that ConversationBufferMemory stores recent exchanges to maintain context, while ConversationBufferWindowMemory keeps a fixed-size window of the most recent interactions to manage memory usage effectively. The AI concludes by asking if James is exploring these concepts for a specific project or application.\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James, and the AI greets him, expressing its willingness to chat and help while inquiring about his day. James mentions he is researching different types of conversational memory, to which the AI responds by explaining that conversational memory involves how systems remember past interactions for a personalized experience. The AI outlines four types of conversational memory: short-term memory, which retains context within a single conversation; long-term memory, which retains information across multiple interactions; contextual memory, which focuses on understanding the flow of conversation; and dynamic memory, which updates based on new information or changes in user preferences. The AI then asks James if he is looking into any specific applications or technologies related to conversational memory. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI finds this interesting and explains that ConversationBufferMemory stores recent exchanges to maintain context, while ConversationBufferWindowMemory keeps a fixed-size window of the most recent interactions to manage memory usage effectively. The AI concludes by asking if James is exploring these concepts for a specific project or application. James notes that buffer memory just stores the entire conversation, prompting the AI to agree and elaborate that ConversationBufferMemory indeed retains the entire conversation history, which is useful for maintaining context but can increase memory usage. The AI contrasts this with ConversationBufferWindowMemory, which efficiently manages memory by keeping only a fixed-size window of recent interactions. The AI then inquires if James is considering the trade-offs between these two types of memory for his research or project.\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
              " 'history': 'The human introduces himself as James, and the AI greets him, expressing its willingness to chat and help while inquiring about his day. James mentions he is researching different types of conversational memory, to which the AI responds by explaining that conversational memory involves how systems remember past interactions for a personalized experience. The AI outlines four types of conversational memory: short-term memory, which retains context within a single conversation; long-term memory, which retains information across multiple interactions; contextual memory, which focuses on understanding the flow of conversation; and dynamic memory, which updates based on new information or changes in user preferences. The AI then asks James if he is looking into any specific applications or technologies related to conversational memory. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI finds this interesting and explains that ConversationBufferMemory stores recent exchanges to maintain context, while ConversationBufferWindowMemory keeps a fixed-size window of the most recent interactions to manage memory usage effectively. The AI concludes by asking if James is exploring these concepts for a specific project or application. James notes that buffer memory just stores the entire conversation, prompting the AI to agree and elaborate that ConversationBufferMemory indeed retains the entire conversation history, which is useful for maintaining context but can increase memory usage. The AI contrasts this with ConversationBufferWindowMemory, which efficiently manages memory by keeping only a fixed-size window of recent interactions. The AI then inquires if James is considering the trade-offs between these two types of memory for his research or project.',\n",
              " 'response': \"That's correct! ConversationBufferWindowMemory indeed retains only the last k messages, which helps in managing memory usage effectively. By dropping older messages, it ensures that the system remains responsive and efficient, especially in scenarios where memory resources are limited. This approach can be particularly useful in applications where maintaining a balance between context and resource management is crucial. Are you thinking about how to implement this in a specific application, or are you more focused on the theoretical aspects of these memory types?\"}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"hello there my name is James\"})\n",
        "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
        "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
        "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
        "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgmdXd2taTR"
      },
      "source": [
        "We can see how the conversation summary varies with each new message. Let's see if the LLM is able to recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHnh4Y1OtaTR",
        "outputId": "deb8f44e-d100-4a6f-81e7-862608bbe3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James, and the AI greets him, expressing its willingness to chat and help while inquiring about his day. James mentions he is researching different types of conversational memory, to which the AI responds by explaining that conversational memory involves how systems remember past interactions for a personalized experience. The AI outlines four types of conversational memory: short-term memory, which retains context within a single conversation; long-term memory, which retains information across multiple interactions; contextual memory, which focuses on understanding the flow of conversation; and dynamic memory, which updates based on new information or changes in user preferences. The AI then asks James if he is looking into any specific applications or technologies related to conversational memory. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI finds this interesting and explains that ConversationBufferMemory stores recent exchanges to maintain context, while ConversationBufferWindowMemory keeps a fixed-size window of the most recent interactions to manage memory usage effectively. The AI concludes by asking if James is exploring these concepts for a specific project or application. James notes that buffer memory just stores the entire conversation, prompting the AI to agree and elaborate that ConversationBufferMemory indeed retains the entire conversation history, which is useful for maintaining context but can increase memory usage. The AI contrasts this with ConversationBufferWindowMemory, which efficiently manages memory by keeping only a fixed-size window of recent interactions. The AI then inquires if James is considering the trade-offs between these two types of memory for his research or project. James clarifies that buffer window memory stores the last k messages, dropping the rest. The AI confirms this, explaining that ConversationBufferWindowMemory retains only the last k messages to manage memory usage effectively, ensuring responsiveness and efficiency, especially in resource-limited scenarios. The AI asks if James is thinking about implementing this in a specific application or focusing more on the theoretical aspects of these memory types.\n",
            "Human: What is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is my name again?',\n",
              " 'history': 'The human introduces himself as James, and the AI greets him, expressing its willingness to chat and help while inquiring about his day. James mentions he is researching different types of conversational memory, to which the AI responds by explaining that conversational memory involves how systems remember past interactions for a personalized experience. The AI outlines four types of conversational memory: short-term memory, which retains context within a single conversation; long-term memory, which retains information across multiple interactions; contextual memory, which focuses on understanding the flow of conversation; and dynamic memory, which updates based on new information or changes in user preferences. The AI then asks James if he is looking into any specific applications or technologies related to conversational memory. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI finds this interesting and explains that ConversationBufferMemory stores recent exchanges to maintain context, while ConversationBufferWindowMemory keeps a fixed-size window of the most recent interactions to manage memory usage effectively. The AI concludes by asking if James is exploring these concepts for a specific project or application. James notes that buffer memory just stores the entire conversation, prompting the AI to agree and elaborate that ConversationBufferMemory indeed retains the entire conversation history, which is useful for maintaining context but can increase memory usage. The AI contrasts this with ConversationBufferWindowMemory, which efficiently manages memory by keeping only a fixed-size window of recent interactions. The AI then inquires if James is considering the trade-offs between these two types of memory for his research or project. James clarifies that buffer window memory stores the last k messages, dropping the rest. The AI confirms this, explaining that ConversationBufferWindowMemory retains only the last k messages to manage memory usage effectively, ensuring responsiveness and efficiency, especially in resource-limited scenarios. The AI asks if James is thinking about implementing this in a specific application or focusing more on the theoretical aspects of these memory types.',\n",
              " 'response': \"Your name is James! How's your research going?\"}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"What is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZpfPyEztaTR"
      },
      "source": [
        "As this information was stored in the summary the LLM successfully recalled our name. This may not always be the case, by summarizing the conversation we inevitably compress the full amount of information and so we may lose key details occasionally. Nonetheless, this is a great memory type for long conversations while retaining some key information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlKvJDUtaTR"
      },
      "source": [
        "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9jdJCkztaTR"
      },
      "source": [
        "Let's implement this memory type using the `RunnableWithMessageHistory` class. As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "C09U1WkqtaTR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages.content,\n",
        "                messages=[x.content for x in messages]\n",
        "            )\n",
        "        )\n",
        "        # replace the existing history with a single system summary message\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "762Vg5u4taTR"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "om78sKCGtaTS"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPfut3S5taTS"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1F1m9n2taTS",
        "outputId": "ff68d98f-f5a0-40f9-e603-38450b2cff23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-a38a0644-74e2-4b97-8e6f-9bd81d2a1369-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqOCHCTtaTS"
      },
      "source": [
        "Let's see what summary was generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDeudJftaTS",
        "outputId": "9e2fe892-1f96-4691-e47a-c0e198913e41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-a38a0644-74e2-4b97-8e6f-9bd81d2a1369-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uLeGXztaTS"
      },
      "source": [
        "Let's continue the conversation and see if the summary is updated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di7WVWmCtaTS",
        "outputId": "df55157b-d642-41b5-b25d-d0b1be47ba75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-a38a0644-74e2-4b97-8e6f-9bd81d2a1369-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That sounds interesting! Conversational memory can refer to various concepts depending on the context, such as in human communication, artificial intelligence, or even in the context of psychology. Here are a few types of conversational memory you might consider:\\n\\n1. **Short-term Memory**: This is the ability to hold a small amount of information in an active state for a brief period. In conversations, this allows individuals to remember what was just said to respond appropriately.\\n\\n2. **Long-term Memory**: This involves storing information over a longer period. In conversations, long-term memory helps individuals recall past interactions, shared experiences, or relevant knowledge that can inform current discussions.\\n\\n3. **Contextual Memory**: This type of memory involves remembering the context in which a conversation took place, including the setting, participants, and emotional tone. It helps in understanding the nuances of a conversation.\\n\\n4. **Episodic Memory**: This is a type of long-term memory that involves the recollection of specific events or experiences. In conversations, it allows individuals to draw on personal anecdotes or past discussions.\\n\\n5. **Semantic Memory**: This refers to the memory of facts and general knowledge. In conversations, semantic memory helps individuals recall information that is relevant to the topic being discussed.\\n\\n6. **Conversational Memory in AI**: In the context of artificial intelligence, conversational memory refers to the ability of a system to remember past interactions with users. This can enhance user experience by allowing for more personalized and context-aware responses.\\n\\n7. **Social Memory**: This involves the collective memory of a group or community, which can influence conversations and shared narratives within that group.\\n\\nAre you looking for information on a specific type of conversational memory, or do you have a particular context in mind?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 354, 'prompt_tokens': 53, 'total_tokens': 407, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-24072554-9c94-4adc-a34f-63061d4dc466-0', usage_metadata={'input_tokens': 53, 'output_tokens': 354, 'total_tokens': 407, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")\n",
        "\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqEv8kxZtaTS"
      },
      "source": [
        "So far so good! Let's continue with a few more messages before returning to the name question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "F7cy8dKZtaTS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOdFy4htaTS"
      },
      "source": [
        "Let's see the latest summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SEBczVutaTS",
        "outputId": "e0f91796-96fa-4daf-8f42-0557525b5bc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-a38a0644-74e2-4b97-8e6f-9bd81d2a1369-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That sounds interesting! Conversational memory can refer to various concepts depending on the context, such as in human communication, artificial intelligence, or even in the context of psychology. Here are a few types of conversational memory you might consider:\\n\\n1. **Short-term Memory**: This is the ability to hold a small amount of information in an active state for a brief period. In conversations, this allows individuals to remember what was just said to respond appropriately.\\n\\n2. **Long-term Memory**: This involves storing information over a longer period. In conversations, long-term memory helps individuals recall past interactions, shared experiences, or relevant knowledge that can inform current discussions.\\n\\n3. **Contextual Memory**: This type of memory involves remembering the context in which a conversation took place, including the setting, participants, and emotional tone. It helps in understanding the nuances of a conversation.\\n\\n4. **Episodic Memory**: This is a type of long-term memory that involves the recollection of specific events or experiences. In conversations, it allows individuals to draw on personal anecdotes or past discussions.\\n\\n5. **Semantic Memory**: This refers to the memory of facts and general knowledge. In conversations, semantic memory helps individuals recall information that is relevant to the topic being discussed.\\n\\n6. **Conversational Memory in AI**: In the context of artificial intelligence, conversational memory refers to the ability of a system to remember past interactions with users. This can enhance user experience by allowing for more personalized and context-aware responses.\\n\\n7. **Social Memory**: This involves the collective memory of a group or community, which can influence conversations and shared narratives within that group.\\n\\nAre you looking for information on a specific type of conversational memory, or do you have a particular context in mind?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 354, 'prompt_tokens': 53, 'total_tokens': 407, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-24072554-9c94-4adc-a34f-63061d4dc466-0', usage_metadata={'input_tokens': 53, 'output_tokens': 354, 'total_tokens': 407, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Great! Both **ConversationBufferMemory** and **ConversationBufferWindowMemory** are concepts often used in the context of conversational AI, particularly in frameworks like LangChain. Here’s a brief overview of each:\\n\\n### ConversationBufferMemory\\n- **Definition**: This type of memory stores the entire conversation history in a buffer. It allows the AI to access all previous messages in the conversation, which can be useful for maintaining context and continuity.\\n- **Use Case**: It is particularly useful in scenarios where the context of the entire conversation is important for generating relevant responses. For example, in a customer support chatbot, having access to the entire conversation can help the bot provide more accurate and context-aware answers.\\n- **Limitations**: The main limitation is that as the conversation grows longer, the memory can become unwieldy, and it may lead to performance issues or increased latency in response times.\\n\\n### ConversationBufferWindowMemory\\n- **Definition**: This type of memory maintains a fixed-size window of the most recent messages in the conversation. Instead of storing the entire conversation, it only keeps a specified number of the latest exchanges.\\n- **Use Case**: This is useful in situations where only the most recent context is relevant for generating responses. For example, in a fast-paced chat environment, it may be sufficient to remember just the last few messages to maintain context without overwhelming the system with too much data.\\n- **Advantages**: It helps manage memory usage more efficiently and can improve response times since the AI only processes a limited amount of information.\\n\\n### Comparison\\n- **Memory Size**: ConversationBufferMemory can grow indefinitely, while ConversationBufferWindowMemory has a fixed size.\\n- **Context Retention**: The former retains all context, which can be beneficial for complex conversations, while the latter focuses on the most recent context, which can be more efficient for simpler or faster interactions.\\n\\n### Considerations\\nWhen choosing between these two types of memory, consider the nature of the conversations your AI will handle. If maintaining a rich context is crucial, ConversationBufferMemory may be the way to go. If efficiency and speed are more important, then ConversationBufferWindowMemory might be more suitable.\\n\\nIf you have specific questions or need further details about implementation or use cases, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 458, 'prompt_tokens': 429, 'total_tokens': 887, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-43572c8c-9071-4753-9385-993606935d14-0', usage_metadata={'input_tokens': 429, 'output_tokens': 458, 'total_tokens': 887, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yes, that's correct! **ConversationBufferMemory** is designed to store the entire conversation history. This means that every message exchanged between the user and the AI is kept in memory, allowing the AI to reference any part of the conversation when generating responses. \\n\\n### Key Features of ConversationBufferMemory:\\n- **Complete History**: It retains all messages, which helps maintain context throughout the conversation, making it easier for the AI to provide relevant and coherent responses.\\n- **Contextual Understanding**: By having access to the entire conversation, the AI can better understand the nuances, references, and emotional tone of the dialogue.\\n- **Use Cases**: This type of memory is particularly useful in scenarios where detailed context is important, such as in customer support, therapy chatbots, or any application where the history of the conversation significantly impacts the interaction.\\n\\n### Limitations:\\n- **Scalability**: As the conversation length increases, the amount of stored data can become large, which may lead to performance issues or slower response times.\\n- **Relevance**: In long conversations, some earlier parts of the dialogue may become less relevant, but they are still retained in memory.\\n\\nIn contrast, **ConversationBufferWindowMemory** only keeps a fixed number of the most recent messages, which can help mitigate some of these limitations by focusing on the most relevant context.\\n\\nIf you have more specific questions about how these memory types work or their applications, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 291, 'prompt_tokens': 902, 'total_tokens': 1193, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-24655ef9-0025-4ae0-b481-ea72fae17d82-0', usage_metadata={'input_tokens': 902, 'output_tokens': 291, 'total_tokens': 1193, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yes, that's correct! **ConversationBufferWindowMemory** is designed to store only the last \\\\( k \\\\) messages of the conversation, effectively creating a sliding window of context. Here’s a more detailed breakdown:\\n\\n### Key Features of ConversationBufferWindowMemory:\\n- **Fixed Size**: It maintains a set limit on the number of messages stored (the last \\\\( k \\\\) messages). Once this limit is reached, the oldest messages are dropped to make room for new ones.\\n- **Recent Context**: This approach allows the AI to focus on the most recent exchanges, which are often the most relevant for generating appropriate responses.\\n- **Efficiency**: By limiting the amount of stored data, it can improve performance and reduce memory usage, making it suitable for applications where quick responses are essential.\\n\\n### Use Cases:\\n- **Fast-Paced Conversations**: In scenarios like chatbots for customer service or social media interactions, where conversations can be rapid and context from earlier messages may not be as critical.\\n- **Real-Time Applications**: Applications that require immediate responses can benefit from this memory type, as it reduces the overhead of processing large amounts of historical data.\\n\\n### Limitations:\\n- **Loss of Context**: Important information from earlier in the conversation may be lost if it falls outside the window of the last \\\\( k \\\\) messages. This can lead to misunderstandings or a lack of continuity in longer conversations.\\n- **Fixed Context**: The AI may not have access to earlier parts of the conversation that could be relevant, which can be a drawback in more complex interactions.\\n\\nIn summary, **ConversationBufferWindowMemory** is a practical solution for managing conversational context efficiently while still retaining enough information to provide relevant responses. If you have any more questions or need further clarification, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 359, 'prompt_tokens': 1214, 'total_tokens': 1573, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-6d8aa0f0-f3a0-41fd-9d59-b432d245cd43-0', usage_metadata={'input_tokens': 1214, 'output_tokens': 359, 'total_tokens': 1573, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcR46iNxtaTS"
      },
      "source": [
        "The information about our name has been maintained, so let's see if this is enough for our LLM to correctly recall our name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doVtbwVytaTS",
        "outputId": "da21bede-7b6a-4b9b-f894-b5dcb1e23bad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James! How can I assist you further today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 1587, 'total_tokens': 1600, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1536}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c3551e24-7018-4904-9106-d59fdceafa12-0', usage_metadata={'input_tokens': 1587, 'output_tokens': 13, 'total_tokens': 1600, 'input_token_details': {'audio': 0, 'cache_read': 1536}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2A8VfZctaTS"
      },
      "source": [
        "Perfect! We've successfully implemented the `ConversationSummaryMemory` type using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJM9TsltaTS"
      },
      "source": [
        "## 4. `ConversationSummaryBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmF85VmAtaTS"
      },
      "source": [
        "Our final memory type acts as a combination of `ConversationSummaryMemory` and `ConversationBufferMemory`. It keeps the buffer for the conversation up until the previous `n` tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
        "\n",
        "\n",
        "```\n",
        "# ~~ a summary of previous interactions\n",
        "The user named James introduced himself and the AI responded, introducing itself as an AI model called Zeta.\n",
        "James then said he was researching the different types of conversational memory and Zeta asked for some\n",
        "examples.\n",
        "# ~~ the most recent messages\n",
        "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "Human: Buffer memory just stores the entire conversation\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tPilUOHtaTT",
        "outputId": "df54e5c6-95fc-4f98-a9ce-7f9956aab907"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z7/20xbnn554f9b6jc8td1jyhjr0000gn/T/ipykernel_71347/569741439.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=300,\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvUBQXmBtaTT"
      },
      "source": [
        "As before, we set up the deprecated memory type using the `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "HdcdtYdrtaTT"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKcrFxOtaTT"
      },
      "source": [
        "First invoke with a single message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAZqJKHGtaTT",
        "outputId": "d117a546-8d2a-43c0-c144-8e1629fafa89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hi, my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Hi, my name is James',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello, James! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Hello, James! It's great to meet you! How's your day going so far?\"}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"Hi, my name is James\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-CwiwztaTT"
      },
      "source": [
        "Looks good so far, let's continue with a few more messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlsR1iaItaTT",
        "outputId": "30e93e9c-215a-48b0-f591-4d33ed0c425e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: I'm researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions that he is researching different types of conversational memory. The AI responds positively and explains that conversational memory allows systems to remember information from past interactions to enhance future conversations. It outlines five types of conversational memory: short-term memory, which retains details from the current conversation; long-term memory, which keeps information across multiple interactions; contextual memory, which remembers the context and emotional tone of discussions; dynamic memory, which updates based on new information; and episodic memory, which recalls specific events or interactions. The AI also notes the challenges related to privacy and data management and asks if James is focusing on a particular type of conversational memory in his research.', additional_kwargs={}, response_metadata={})]\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions that he is researching different types of conversational memory. The AI responds positively and explains that conversational memory allows systems to remember information from past interactions to enhance future conversations. It outlines five types of conversational memory: short-term memory, which retains details from the current conversation; long-term memory, which keeps information across multiple interactions; contextual memory, which remembers the context and emotional tone of discussions; dynamic memory, which updates based on new information; and episodic memory, which recalls specific events or interactions. The AI also notes the challenges related to privacy and data management and asks if James is focusing on a particular type of conversational memory in his research.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That sounds interesting, James! ConversationBufferMemory and ConversationBufferWindowMemory are both fascinating types of conversational memory. \\n\\n**ConversationBufferMemory** typically retains a fixed-size buffer of the most recent interactions. This means it can remember a certain number of past exchanges, which helps maintain context in ongoing conversations. It's particularly useful for applications where the immediate context is crucial, like chatbots that need to respond based on the last few messages.\\n\\nOn the other hand, **ConversationBufferWindowMemory** is a variation that focuses on a sliding window of memory. Instead of just keeping a fixed number of interactions, it can adjust the size of the memory based on the context or the importance of the conversation. This allows for more flexibility, as it can prioritize more relevant information while discarding less important details.\\n\\nBoth types have their advantages and can be tailored to different conversational needs. Are you exploring how these types of memory can be applied in specific scenarios or applications?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions that he is researching different types of conversational memory. The AI responds positively, explaining that conversational memory enhances future conversations by remembering past interactions and outlining five types: short-term, long-term, contextual, dynamic, and episodic memory. It also notes challenges related to privacy and data management, asking if James is focusing on a particular type in his research. James mentions he is looking at ConversationBufferMemory and ConversationBufferWindowMemory, to which the AI responds that both are fascinating. It explains that ConversationBufferMemory retains a fixed-size buffer of recent interactions for context, while ConversationBufferWindowMemory uses a sliding window to adjust memory size based on context or importance, allowing for flexibility. The AI inquires if James is exploring specific applications for these memory types.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's a great point, James! Buffer memory, in general, does indeed store the entire conversation up to a certain limit, allowing the system to reference previous exchanges. This can be particularly useful for maintaining context and continuity in conversations, especially in scenarios where users might refer back to earlier parts of the dialogue.\\n\\nHowever, the way it stores this information can vary. For instance, **ConversationBufferMemory** keeps a fixed number of recent interactions, while **ConversationBufferWindowMemory** can adapt based on the context, potentially allowing for a more dynamic approach to memory management. \\n\\nIn practical applications, this means that while both types can retain the conversation history, the way they prioritize and manage that information can lead to different user experiences. Are you considering any specific applications where this type of memory would be particularly beneficial?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    chain.invoke({\"input\": msg})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Rj5SMftaTT"
      },
      "source": [
        "We can see with each new message the initial `SystemMessage` is updated with a new summary of the conversation. This initial `SystemMessage` is then followed by the most recent `AIMessage` and `HumanMessage` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Dnn_N8taTT"
      },
      "source": [
        "### `ConversationSummaryBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72St8qDjtaTT"
      },
      "source": [
        "As with the previous memory types, we will implement this memory type again using the `RunnableWithMessageHistory` class. In our implementation we will modify the buffer window to be based on the number of messages rather than number of tokens. This tweak will make our implementation more closely aligned with original buffer window.\n",
        "\n",
        "We will implement all of this via a new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tWOvrR4wtaTT"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we\n",
        "        drop.\n",
        "        \"\"\"\n",
        "        existing_summary: SystemMessage | None = None\n",
        "        old_messages: list[BaseMessage] | None = None\n",
        "        # see if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            print(\">> Found existing summary\")\n",
        "            existing_summary = self.messages.pop(0)\n",
        "        # add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "        # check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            print(\n",
        "                f\">> Found {len(self.messages)} messages, dropping \"\n",
        "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "            # pull out the oldest messages...\n",
        "            old_messages = self.messages[:self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "        if old_messages is None:\n",
        "            print(\">> No old messages to update summary with\")\n",
        "            # if we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary = \" \".join([x.content for x in self.messages]),\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "        print(f\">> New summary: {new_summary.content}\")\n",
        "        # prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITlLoeAtaTT"
      },
      "source": [
        "Redefine the `get_chat_history` function to use our new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rvWsP1GMtaTT"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatOpenAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HbocfKdtaTT"
      },
      "source": [
        "Setup our pipeline with new configurable fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cQ_-21_3taTU"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10GMrRXutaTU"
      },
      "source": [
        "Finally, we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfJn7DKtaTU",
        "outputId": "397a9a9c-9469-44ea-d3a5-82d8f2cab3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-f185fb58-a405-4e36-84e0-b73dcd358466-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        ")\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mODJviTtaTU",
        "outputId": "fc1fb499-ab3e-426c-fe48-108da13d14e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Message 1\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 2\n",
            "---\n",
            "\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: In this conversation, James introduces himself and expresses interest in researching different types of conversational memory. The AI responds positively, explaining that conversational memory can encompass various concepts across human communication, artificial intelligence, and psychology. The AI outlines several types of conversational memory:\n",
            "\n",
            "1. **Short-term Memory**: Retains a small amount of information for brief periods, aiding immediate responses.\n",
            "2. **Long-term Memory**: Stores information over longer durations, including past interactions and personal details.\n",
            "3. **Contextual Memory**: Recalls the context of conversations, influencing interpretation.\n",
            "4. **Episodic Memory**: Involves recollection of specific events or experiences relevant to discussions.\n",
            "5. **Semantic Memory**: Encompasses facts and general knowledge for informed discussions.\n",
            "6. **Social Memory**: Remembers social interactions and relationships, shaping future conversations.\n",
            "7. **Conversational Memory in AI**: Refers to AI's ability to remember past interactions for personalized responses.\n",
            "\n",
            "The AI invites James to explore specific areas of conversational memory further. James then mentions he has been looking into **ConversationBufferMemory** and **ConversationBufferWindowMemory**, which the AI elaborates on, explaining their definitions, use cases, and limitations. **ConversationBufferMemory** stores the entire conversation history for context, while **ConversationBufferWindowMemory** keeps a fixed-size window of recent messages for efficiency. The AI concludes by summarizing the strengths and weaknesses of both types of memory in conversational AI.\n",
            "---\n",
            "Message 3\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: In this conversation, the user is researching different types of conversational memory, specifically focusing on **ConversationBufferMemory** and **ConversationBufferWindowMemory**. The AI provides an overview of these two concepts used in conversational AI frameworks:\n",
            "\n",
            "### ConversationBufferMemory\n",
            "- **Definition**: Stores the entire conversation history, allowing the AI to access all previous messages.\n",
            "- **Use Case**: Ideal for applications requiring full context, such as customer support.\n",
            "- **Limitations**: Can become resource-intensive and unwieldy in long conversations, potentially leading to performance issues.\n",
            "\n",
            "### ConversationBufferWindowMemory\n",
            "- **Definition**: Maintains a fixed-size window of recent messages instead of the entire conversation.\n",
            "- **Use Case**: Useful when only the most recent context is relevant, improving memory management and performance.\n",
            "- **Limitations**: May lose important earlier context, leading to less coherent responses if relevant information falls outside the window.\n",
            "\n",
            "The AI emphasizes that the choice between these two types of memory depends on the application's specific requirements and the importance of context.\n",
            "\n",
            "Additionally, the user expresses interest in exploring various types of conversational memory, prompting the AI to outline several categories, including:\n",
            "1. **Short-term Memory**: Retains information for brief periods.\n",
            "2. **Long-term Memory**: Stores information over longer durations.\n",
            "3. **Contextual Memory**: Recalls the context of conversations.\n",
            "4. **Episodic Memory**: Remembers specific events or experiences.\n",
            "5. **Semantic Memory**: Holds facts and general knowledge.\n",
            "6. **Social Memory**: Remembers social interactions and relationships.\n",
            "7. **Conversational Memory in AI**: Refers to the ability of AI systems to remember past interactions for personalized responses.\n",
            "\n",
            "The AI invites further questions on specific areas of conversational memory, indicating a willingness to assist with deeper exploration or implementation details.\n",
            "---\n",
            "Message 4\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: In this conversation, the user is exploring two types of memory management in conversational AI: **ConversationBufferMemory** and **ConversationBufferWindowMemory**. \n",
            "\n",
            "### Key Points Discussed:\n",
            "\n",
            "1. **ConversationBufferMemory**:\n",
            "   - **Definition**: This memory type stores the entire conversation history, allowing the AI to access all previous messages.\n",
            "   - **Key Features**:\n",
            "     - **Complete Context**: Retains all messages for a comprehensive understanding, useful in complex dialogues.\n",
            "     - **Flexibility**: The AI can refer back to any part of the conversation for accurate responses.\n",
            "     - **Use Cases**: Ideal for applications like customer support where full interaction history is crucial.\n",
            "   - **Considerations**:\n",
            "     - **Resource Intensive**: Can consume significant memory and processing power, especially in long conversations.\n",
            "     - **Potential for Overload**: Lengthy conversations may lead to challenges in processing and retrieving relevant information.\n",
            "\n",
            "2. **ConversationBufferWindowMemory**:\n",
            "   - **Definition**: Maintains a fixed-size window of the most recent messages, discarding older ones beyond a defined limit.\n",
            "   - **Key Features**:\n",
            "     - **Fixed Size**: Only stores a specific number of recent messages.\n",
            "     - **Efficiency**: More resource-efficient, improving performance by limiting data processing.\n",
            "     - **Use Cases**: Useful in fast-paced conversations where only recent context is relevant.\n",
            "   - **Considerations**:\n",
            "     - **Loss of Context**: Important earlier context may be lost, potentially leading to less coherent responses.\n",
            "\n",
            "### Summary:\n",
            "The conversation highlights the strengths and weaknesses of both memory types. **ConversationBufferMemory** is best for scenarios requiring full context, while **ConversationBufferWindowMemory** is more efficient for managing memory and performance in longer dialogues. The choice between them depends on the specific application needs and the importance of context in the conversation. The user is encouraged to ask further questions or seek clarification on these memory types.\n"
          ]
        }
      ],
      "source": [
        "for i, msg in enumerate([\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]):\n",
        "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_Xx40HtaTU"
      },
      "source": [
        "There we go, we've successfully implemented the `ConversationSummaryBufferMemory` type using `RunnableWithMessageHistory`!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
